{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "YH0ehjW16rhk"
   },
   "outputs": [],
   "source": [
    "def load_images(tlabels):\n",
    "  train = np.load(\"/content/gdrive/My Drive/notebooks/train_images.npy\", encoding = 'bytes')\n",
    "  test = np.load(\"/content/gdrive/My Drive/notebooks/test_images.npy\", encoding = 'bytes')\n",
    "  train_labels = pd.read_csv(\"/content/gdrive/My Drive/notebooks/train_labels.csv\")\n",
    "\n",
    "  x_train = train[:,1]\n",
    "  x_test = test[:,1]\n",
    "  for i in range(len(x_train)):\n",
    "    x_train[i] = (x_train[i] - np.min(x_train[i]))/(np.max(x_train[i] - np.min(x_train[i])))\n",
    "  for i in range(len(x_test)):          \n",
    "    x_test[i] = (x_test[i] - np.amin(x_test[i]))/(np.amax(x_test[i] - np.amin(x_test[i])))\n",
    "\n",
    "  train_labs = pd.factorize(train_labels['Category'])\n",
    "  train_lab = train_labs[0]\n",
    "\n",
    "  x_train = np.array([np.array(x) for x in x_train])\n",
    "  x_test = np.array([np.array(x) for x in x_test])\n",
    "\n",
    "  x_train = x_train.reshape((10000,100,100))\n",
    "  x_test = x_test.reshape((10000,100,100))\n",
    "\n",
    "  lb = LabelBinarizer()\n",
    "  y_train = lb.fit_transform(train_lab)\n",
    "  x_train,y_train = shuffle(x_train,y_train, random_state = random.randint(1,21)*5)\n",
    "\n",
    "  train_X = x_train[:8000]\n",
    "  train_y = y_train[:8000]\n",
    "  valid_X = x_train[8000:]\n",
    "  valid_y = y_train[8000:]\n",
    "  test_X = x_test\n",
    "\n",
    "  print(train_X.shape,valid_X.shape,valid_y.shape)\n",
    "  \n",
    "  if tlabels:\n",
    "    return train_X,train_y,valid_X,valid_y, test_X,train_labels\n",
    "  \n",
    "  return train_X,train_y,valid_X,valid_y, test_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "9jl5n9JX6rhn"
   },
   "outputs": [],
   "source": [
    "def denoise(noisy,s1,s2,s3,s4):\n",
    "  se1 = cv2.getStructuringElement(cv2.MORPH_RECT, (s1,s1))\n",
    "  se2 = cv2.getStructuringElement(cv2.MORPH_RECT, (s2,s2))\n",
    "  se3 = cv2.getStructuringElement(cv2.MORPH_RECT, (s3,s3))\n",
    "  se4 = cv2.getStructuringElement(cv2.MORPH_RECT, (s4,s4))\n",
    "  mask = cv2.morphologyEx(noisy, cv2.MORPH_CLOSE, se1)\n",
    "  mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, se2)\n",
    "  mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, se3)\n",
    "  mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, se4)\n",
    "\n",
    "  out = (noisy * mask)/255\n",
    "  return out\n",
    "\n",
    "\n",
    "def horizontal_flip(image_array: np.ndarray):\n",
    "  # horizontal flip doesn't need skimage, it's easy as flipping the image array of pixels !\n",
    "  return image_array[:, ::-1]\n",
    "\n",
    "def tf_resize_images(X_img_file_paths):\n",
    "  X_data = []\n",
    "  tf.reset_default_graph()\n",
    "  X = tf.placeholder(tf.float32, (None, None, 1))\n",
    "  tf_img = tf.image.resize_images(X, (100, 100), \n",
    "                                  tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "  with tf.Session() as sess:\n",
    "      sess.run(tf.global_variables_initializer())\n",
    "\n",
    "      # Each image is resized individually as different image may be of different size.\n",
    "      for index, file_path in tqdm(enumerate(X_img_file_paths),desc=\"resizing train\"):\n",
    "          img = X_img_file_paths[index]\n",
    "          resized_img = sess.run(tf_img, feed_dict = {X: img})\n",
    "          X_data.append(resized_img)\n",
    "\n",
    "  X_data = np.array(X_data, dtype = np.float32) # Convert to numpy\n",
    "  return X_data\n",
    "\n",
    "def tf_rotate_images(X_img_file_paths):\n",
    "  X_data = []\n",
    "  tf.reset_default_graph()\n",
    "  X = tf.placeholder(tf.float32, (None, None, 1))\n",
    "  degrees = tf.random_uniform([], -25, 25)\n",
    "  tf_img = tf.contrib.image.rotate(X, degrees, interpolation='BILINEAR')\n",
    "\n",
    "  tf_img = tf.image.resize_images(X, (100, 100), \n",
    "                                  tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "  with tf.Session() as sess:\n",
    "      sess.run(tf.global_variables_initializer())\n",
    "\n",
    "      # Each image is resized individually as different image may be of different size.\n",
    "      for index, file_path in tqdm(enumerate(X_img_file_paths),desc=\"rotating train\"):\n",
    "          img = X_img_file_paths[index]\n",
    "          resized_img = sess.run(tf_img, feed_dict = {X: img})\n",
    "          X_data.append(resized_img)\n",
    "\n",
    "  X_data = np.array(X_data, dtype = np.float32) # Convert to numpy\n",
    "  return X_data\n",
    "\n",
    "def images_preprocessing(images):\n",
    "      list_hog_fd = []\n",
    "      for feature in tqdm(images):\n",
    "              fd = hog(feature, orientations=9, pixels_per_cell=(50, 50), cells_per_block=(1, 1))\n",
    "              list_hog_fd.append(fd)\n",
    "      return np.array(list_hog_fd, 'float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "B-mp5S7e6rhp"
   },
   "outputs": [],
   "source": [
    "def data_augmentation(X, y = None, denoise_mask1 = False, denoise_mask2 = False, rand_rotate=False, rand_resize=False, horiz_flip = False):\n",
    "  \n",
    "  if denoise_mask1 :\n",
    "    tr1_X = X.copy()\n",
    "    for i in tqdm(range(len(X)),desc='denoising train 1'):\n",
    "      tr1_X[i] = denoise(tr1_X[i],7,5,5,1)\n",
    "    X = np.row_stack((X,tr1_X))\n",
    "    if y.any():\n",
    "      y = np.row_stack((y,y))\n",
    "                           \n",
    "  if denoise_mask2 :\n",
    "    tr2_X = X.copy()\n",
    "    for i in tqdm(range(len(X)),desc='denoising train 2'):\n",
    "      tr2_X[i] = denoise(tr2_X[i],6,4,4,1)\n",
    "    X = np.row_stack((X,tr2_X))\n",
    "    if y.any():\n",
    "      y = np.row_stack((y,y))\n",
    " \n",
    "  if horiz_flip : \n",
    "    tr_hor_flip = horizontal_flip(X)\n",
    "    X = np.row_stack((X,tr_hor_flip))\n",
    "    y = np.row_stack((y,y))\n",
    "   \n",
    "  X = reshape_dataset(X)\n",
    "  \n",
    "  if rand_rotate : \n",
    "    tr_rotate = tf_rotate_images(X.reshape(-1, 100, 100, 1))\n",
    "    X = np.row_stack((X,tr_rotate))\n",
    "    y = np.row_stack((y,y))\n",
    "\n",
    "  if rand_resize:\n",
    "    tr_resize = tf_resize_images(X.reshape(-1, 100, 100, 1))\n",
    "    X = np.row_stack((X,tr_resize))\n",
    "    y = np.row_stack((y,y))\n",
    "\n",
    "  return shuffle(X,y, random_state = random.randint(1,21)*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "VHavhLu46rhs"
   },
   "outputs": [],
   "source": [
    "def reshape_dataset(X):\n",
    "  # Reshape training , validation and testing image\n",
    "  X = X.reshape(-1, 100, 100, 1)\n",
    "  return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "EqxknNLJ6rhy"
   },
   "outputs": [],
   "source": [
    "def compatible_convolutional_noise_shape(Y):\n",
    "    noiseshape = tf.shape(Y)\n",
    "    noiseshape = noiseshape * tf.constant([1,0,0,1]) + tf.constant([0,1,1,0])\n",
    "    return noiseshape\n",
    "\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    training = tf.placeholder(tf.bool)\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.layers.batch_normalization(x)\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "\n",
    "    return tf.nn.relu6(x) \n",
    "\n",
    "def avgpool2d(x, k=2):\n",
    "    return tf.nn.avg_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],padding='SAME')\n",
    "  \n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],padding='SAME')\n",
    "  \n",
    "def batchnorm(Ylogits, iteration, offset, convolutional=False):\n",
    "    exp_moving_avg = tf.train.ExponentialMovingAverage(0.999, iteration) # adding the iteration prevents from averaging across non-existing iterations\n",
    "    bnepsilon = 1e-5\n",
    "    print(Ylogits)\n",
    "    if convolutional:\n",
    "        mean, variance = tf.nn.moments(Ylogits, [0, 1, 2])\n",
    "    else:\n",
    "        mean, variance = tf.nn.moments(Ylogits, [0])\n",
    "    update_moving_everages = exp_moving_avg.apply([mean, variance])\n",
    "    m = tf.cond(is_test, lambda: exp_moving_avg.average(mean), lambda: mean)\n",
    "    v = tf.cond(is_test, lambda: exp_moving_avg.average(variance), lambda: variance)\n",
    "    Ybn = tf.nn.batch_normalization(Ylogits, m, v, offset, None, bnepsilon)\n",
    "    return Ybn, update_moving_everages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "SBtryO6fOSaB"
   },
   "outputs": [],
   "source": [
    "def initialize_filters(filter_sizes,num_filters,final_size,init):\n",
    "  initializer = None\n",
    "  if 'xavier' in init:\n",
    "    initial=tf.contrib.layers.xavier_initializer()\n",
    "  else:\n",
    "    initial=tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "  weights = {'wc'+str(i+1): tf.get_variable('W'+str(i), shape=(filter_sizes[i][0],filter_sizes[i][1],num_filters[i],num_filters[i+1]), initializer = initial)\n",
    "             for i in range(len(num_filters)-1)}\n",
    "  biases = {'bc'+str(i+1):tf.get_variable('B'+str(i), shape=(num_filters[i+1]), initializer=initial) for i in range(len(num_filters)-1)}\n",
    "\n",
    "  weights['wd1'] = tf.get_variable('W'+str(len(num_filters)), shape=(final_size*final_size*num_filters[len(num_filters)-1],num_filters[len(num_filters)-1]), initializer = initial)\n",
    "  weights['out'] = tf.get_variable('W'+str(len(num_filters)+1), shape=(num_filters[len(num_filters)-1],n_classes), initializer = initial)\n",
    "  biases['bd1'] = tf.get_variable('B'+str(len(num_filters)), shape=(num_filters[len(num_filters)-1]), initializer = initial)\n",
    "  biases['out'] = tf.get_variable('B'+str(len(num_filters)+1), shape=(n_classes), initializer = initial)\n",
    "  \n",
    "  return weights,biases\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ef9dN9XMeMkb"
   },
   "outputs": [],
   "source": [
    "def cnn(x,weights,biases,strides,dropout,drop = 0):\n",
    "  conv = None\n",
    "  for i in range(len(weights)-2):\n",
    "    conv = conv2d(x, weights['wc'+str(i+1)], biases['bc'+str(i+1)])\n",
    "    print(conv)\n",
    "    conv = maxpool2d(conv, k=strides)\n",
    "    print(conv)\n",
    "    x = conv\n",
    "  # Reshape conv2 output to fit fully connected layer input\n",
    "  fc1 = tf.reshape(conv, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "  print(fc1)\n",
    "\n",
    "  fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "  fc1 = tf.nn.relu(fc1)\n",
    "  # Output, class prediction\n",
    "  # finally we multiply the fully connected layer with the weights and add a bias term. \n",
    "  out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "  print(out)\n",
    "\n",
    "  if drop:\n",
    "    drop = tf.layers.dropout(inputs=out, rate=drop)\n",
    "    print(drop)\n",
    "    return drop\n",
    "\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "tcTaL3f66riA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_session():\n",
    "    with tf.Session() as sess:\n",
    "        init=tf.global_variables_initializer()\n",
    "        sess.run(init) \n",
    "        train_loss = []\n",
    "        valid_loss = []\n",
    "        train_accuracy = []\n",
    "        valid_accuracy = []\n",
    "        train_writer = tf.summary.FileWriter(DIR + '/train',sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(DIR + '/test')\n",
    "        for i in range(training_iters):\n",
    "#               if i%20 == 0:\n",
    "#             train_X, train_y = shuffle(train_X,train_y, random_state = i)\n",
    "            for batch in tqdm(range(len(train_X)//batch_size)):\n",
    "                batch_x = train_X[batch*batch_size:min((batch+1)*batch_size,len(train_X))]\n",
    "                batch_y = train_y[batch*batch_size:min((batch+1)*batch_size,len(train_y))]   \n",
    "                # Run optimization op (backprop).\n",
    "                    # Calculate batch loss and accuracy\n",
    "                opt = sess.run(optimizer, feed_dict={x: batch_x,\n",
    "                                                                  y: batch_y})\n",
    "                summary, loss, acc = sess.run([merged,cost, accuracy], feed_dict={x: batch_x,\n",
    "                                                                  y: batch_y})\n",
    "                train_writer.add_summary(summary, i)\n",
    "\n",
    "\n",
    "            print(\"Iter \" + str(i) + \", Loss= \" + \\\n",
    "                          \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.5f}\".format(acc))\n",
    "            print(\"Optimization Finished!\")\n",
    "            summary, valid_acc = sess.run([merged,accuracy], feed_dict={x:valid_X, y:valid_y})\n",
    "            test_writer.add_summary(summary, i)\n",
    "\n",
    "            train_loss.append(loss)\n",
    "            valid_loss.append(valid_loss)\n",
    "            train_accuracy.append(acc)\n",
    "            valid_accuracy.append(valid_acc)\n",
    "            print(\"Validation Accuracy:\",\"{:.5f}\".format(valid_acc))\n",
    "        predict = sess.run([predict], feed_dict={x: test_X})\n",
    "        summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6199
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "nOGudyrE1biV",
    "outputId": "011768cd-bee5-443b-f866-e3c0a31ef820"
   },
   "outputs": [],
   "source": [
    "train_X,train_y,valid_X,valid_y, test_X = load_images(False)\n",
    "train_X = reshape_dataset(train_X)\n",
    "valid_X = reshape_dataset(valid_X)\n",
    "test_X = reshape_dataset(test_X)\n",
    "\n",
    "training_iters = 400 \n",
    "learning_rate = 0.001 \n",
    "batch_size = 128\n",
    "\n",
    "# Kaggle data input (img shape: 100*100)\n",
    "n_input = 100\n",
    "\n",
    "# Kaggle total classes (31 classes)\n",
    "n_classes = 31\n",
    "\n",
    "#both placeholders are of type float\n",
    "x = tf.placeholder(\"float\", [None, 100,100,1])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "weights,biases = initialize_filters([(8,8),(8,8),(8,8),(4,4),(4,4),(2,2)],[1,8,16,32,64,128,256],2,'xavier')\n",
    "pred = cnn(x,weights,biases,2,True,0.4)\n",
    "\n",
    "with tf.name_scope(\"cost\"):\n",
    "  cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))\n",
    "  tf.summary.scalar('loss', cost)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "predict = tf.argmax(pred, 1)\n",
    "\n",
    "#Here you check whether the index of the maximum value of the predicted image is equal to the actual labelled image. and both will be a column vector.\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "#calculate accuracy across all the given images and average them out. \n",
    "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "  tf.summary.scalar('accuracy', accuracy)\n",
    "  \n",
    "# Initializing the variables\n",
    "sess = tf.Session()\n",
    "DIR = \"/content/drive/My Drive/Colab Notebooks/\"\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "run_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "cnJQcobudZmm"
   },
   "outputs": [],
   "source": [
    "den_tr_X, den_tr_y = data_augmentation(train_X.reshape((8000,100,100)),train_y,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ZxKFybFwaULB"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(train_X[0].reshape((100,100)),cmap='binary_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "dH9lZF5ckaJO"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(den_tr_X[8000].reshape((100,100)),cmap='binary_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "9mapSsHbwz8K"
   },
   "outputs": [],
   "source": [
    "|def cross_validate(session, split_size=5):\n",
    "  results = []\n",
    "  kf = KFold(n_splits=split_size)\n",
    "  for train_idx, val_idx in kf.split(train_X, train_y):\n",
    "    x_train = train_X[train_idx]\n",
    "    y_train = train_y[train_idx]\n",
    "    val_x = train_X[val_idx]\n",
    "    val_y = train_y[val_idx]\n",
    "    run_train(session, train_X, train_y)\n",
    "    results.append(session.run(accuracy, feed_dict={x: val_x, y: val_y}))\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "l8hBixJd6riD"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "myFile = open('submission-final1.csv', 'w')  \n",
    "with myFile:  \n",
    "    writer = csv.writer(myFile)\n",
    "    writer.writerow(['Id','Category'])\n",
    "    for i in range(len(train_labs[0])):\n",
    "        writer.writerow([i,train_labs[1][predict[0][i]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "K9pwHk1oN-gC",
    "outputId": "fc8a7fab-63aa-424f-fe55-8eef3fff24ad"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "train_X,train_y,valid_X,valid_y, test_X,train_labels = load_images(True)\n",
    "pca = PCA(n_components=2,whiten=True)\n",
    "train_X = pca.fit_transform(train_X.reshape((8000,10000)))\n",
    "valid_X = pca.transform(valid_X.reshape((2000,10000)))\n",
    "svc = LinearSVC(random_state=0, tol=1e-5)\n",
    "svc.fit(train_X.reshape(8000,2), train_labels['Category'][:8000])\n",
    "z = svc.predict(valid_X.reshape((2000,2)))\n",
    "f1 = z[z == train_labels['Category'][8000:]]\n",
    "len(f1)/len(train_labels['Category'][8000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "TS9qvJKpupFt",
    "outputId": "81b57569-88e5-4d35-a46d-f2003a3f433d"
   },
   "outputs": [],
   "source": [
    "y = pd.factorize(train_labels.iloc[:8000,1])[0].astype(np.uint16)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(train_X[:,0],train_X[:,1],c=y,cmap='viridis')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cnn_devoir4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
